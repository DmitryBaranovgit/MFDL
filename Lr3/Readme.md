# Лабораторная работа №3: Реализация алгоритма обратного распространения ошибки

## Цель работы
Изучить и реализовать алгоритм обратного распространения ошибки для обучения многослойной полносвязной нейронной сети на языке Python с использование объектно-ориентированного подхода.

## Реализовано

1. **Реализованы функции активации** и их производные:
- Сигмоида (`sigmoid`)
- ReLU (`relu`)
- tanh (`tanh`)

2. **Класс `NuralNetwork`**, реализующий:
- Инициализацию весов и смещений для каждого слоя
- Прямое распространение сигнала (forward propagation)
- Вычисление функции потерь (кросс-энтропия)
- Алгоритм обратного распространения ошибки (backpropagation)
- Обновление параметров с использованием градиентного спуска

3. **Поддержка нескольких слоев** и разных функций активации, что делает сеть гибкой для различных задач.

4. **Использованы словари** для хранения параметров, кешей и градиентов на каждом шаге.

## Результат

В ходе лабораторной работы была построена и обучена простая полносвязная нейронная сеть, способная обучаться на наборе данных с использованием реализованного алгоритма обратного распространения ошибки.

# Генерация данных "Две луны"

![Сгенерированные данные (две луны)](images/Generated_data_(two_moons).png)

# Определение архитекутры и обучение сети

![Обучение сети](images/Defining_the_architecture_and_training_the_network.png)

# Визуализация функции потерь

![Функция потерь во время обучения](images/Loss_function_during_training.png)

# Визуализации границы решений

![Граница решений нейронной сети](images/Neural_network_decision_boudary.png)

# Оценка точности (Accuracy)

![Accuracy](images/Accuracy.png)

## Выводы

- Алгоритм backpropogation позвоялет эффективног обучать многослойные нейросети путем вычисления градиентов ошибки по слоям.
- Такая реализация помогает лучше понять внутрении процессы обучения нейросетей, включая взаимодействие слоев, роль производных и обновление параметров.
